{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE-Based Mapping\n",
    "\n",
    "Compute mappings between sentences and shor text answers, using different variants of ROUGE and a distance heuristic to pick the sentence closest to the shell noun sentence in case of ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mapping_util import long_to_wide, get_prediction\n",
    "\n",
    "annotators = [\"A1\", \"A2\", \"A3\", \"A4\", \"A5\"]\n",
    "variants = [\"rouge1\", \"rouge2\", \"rouge3\", \"rougeL\"]\n",
    "measures = [\"precision\", \"recall\", \"fmeasure\"]\n",
    "\n",
    "\n",
    "nyt = pd.read_csv(\"data/gold_data_mapped.csv\", index_col=[\"batch\", \"file\"])\n",
    "nyt = nyt.groupby(level=[0, 1]).apply(\n",
    "    lambda f: f.reset_index(drop=True).rename_axis(\"sent_index\")\n",
    ")\n",
    "gold = nyt[nyt.is_antecedent].index.get_level_values(\"sent_index\").to_series()\n",
    "\n",
    "answers = pd.read_csv(\n",
    "    \"data/clean_answers.csv\",\n",
    "    index_col=[\"batch\", \"file\"],\n",
    ")\n",
    "\n",
    "answers = long_to_wide(answers, 5, annotators).answer\n",
    "\n",
    "df = (\n",
    "    nyt[[\"shellnoun\", \"sentence\", \"is_sn_sent\"]]\n",
    "    .join(answers)\n",
    "    .set_index([\"shellnoun\", \"is_sn_sent\"], append=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(variants, use_stemmer=True)\n",
    "\n",
    "vec_score = np.vectorize(scorer.score)\n",
    "\n",
    "rouge_results = pd.DataFrame(\n",
    "    vec_score(\n",
    "        df.filter(like=\"A\").map(str.lower), df.sentence.str.lower().values[:, None]\n",
    "    ),\n",
    "    index=df.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute antecedent sentence predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "\n",
    "def evaluate_(rouge_variant, measure):\n",
    "    \"\"\"compute prediction accuracy\"\"\"\n",
    "    scores = rouge_results.map(itemgetter(rouge_variant)).map(attrgetter(measure))\n",
    "    prediction = get_prediction(scores)\n",
    "\n",
    "    return prediction.eq(gold.values).mean()\n",
    "\n",
    "\n",
    "evaluate = np.vectorize(evaluate_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge1</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge3</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        precision  recall  fmeasure\n",
       "rouge1       0.58    0.67      0.66\n",
       "rouge2       0.65    0.68      0.69\n",
       "rouge3       0.59    0.64      0.61\n",
       "rougeL       0.58    0.65      0.66"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(\n",
    "    evaluate(np.array(variants)[:, None], measures), index=variants, columns=measures\n",
    ")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering-9qBh_5qX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
